---
title: "simulation"
output: html_document
---

One powerful tool in an analysts toolbox is simulation. If you're ever unsure about whether you can do a certain procedure or if modeling your data in a certain way is a good or bad idea, you can often get your answer through running a simulation. Unfortunately, I don't think psychologists, in general, receive the training needed to incorporate this into their workflow. That's too bad, because it really is powerful. So, this is a primer on how to simulate. We'll start with a relatively easy example.

Suppose we run an experiment with a control group and an experimental group. If you read a statistics textbook, they'll tell you that this can be described with an equation like this:

$$
y_i = \beta_0 + \beta_{1}X_i + \epsilon_i
$$

Looks scary, right? There are latin letters, greek letters, and subscripts. What's all that mean?

Well, $y$ is the response, also known as the dependent variable. It's the thing you're looking to explain. You ran an experiment, and saw some outcome and you're interested in what produced that outcome (hint - you manipulated some variable, right?). There's a subscript to indicate that there are multiple instances of $y$. In fact, if there are 4 observations of your dependent variable, then:

$$i = 1, 2, 3, 4$$

Let's take a simple example. Here are your 4 observed values for $y$

```{r}
y <- c(2, 4, 8, 10)
cat(y, sep='\n')
```

In this example, what is $y_i$ for $i = 2$? The way we access values in R works similarly to this notation. Let's get $y_2$: 

```{r}
y[2]
```

Now, we specified before that we ran an experiment with two groups. This is also represented in the equation above. Specifically, $X$ will represent our manipulation. You'll notice it is also denoted with a subscript $i$, which means that it will have the same number of instances as there are in $y$. Since there are two groups, $X$ will take on two values. There are a number of ways of representing these two groups numerically, but we'll use 0 for control group and 1 for experimental group. Let's return to our y variable. Which of these 4 observations came from the control group and which came from the experimental group?

```{r}
cat(y, sep='\n')
```

We haven't specified this until now, and it doesn't really matter for this toy example which observation goes with which group. Because the latter two numbers of the y *vector* are much larger, we'll say that they came from the experimental group - this is what a big effect looks like!

Thus, that means that this is our x:

```{r}
x <- c(0, 0, 1, 1)
cat(x, sep='\n')
```

So, we can plug these values into our equation above and we now have a series of equations that describe each outcome:

$$
2 = \beta_0 + \beta_{1}*0 + \epsilon_i \\
4 = \beta_0 + \beta_{1}*0 + \epsilon_i \\
8 = \beta_0 + \beta_{1}*1 + \epsilon_i \\
10 = \beta_0 + \beta_{1}*1 + \epsilon_i \\
$$

Now, we need to make these equations work mathematically. So first, we see that there's some spots where something is being multiplied by zero. That particular term, then, is zero, so let's simplify:

$$
2 = \beta_0 + \epsilon_i \\
4 = \beta_0 + \epsilon_i \\
8 = \beta_0 + \beta_{1}*1 + \epsilon_i \\
10 = \beta_0 + \beta_{1}*1 + \epsilon_i \\
$$

We're getting pretty close now. Let's first tackle this $\beta_0$. This term is typically called the intercept or the constant. Because we have chosen to use a dummy code for $X$ (i.e. zero and one), that means that the intercept is the value $y$ takes when $X$ is zero. That means we're looking at the values for $y_1$ and $y_2$. They aren't identical, if you take their average, that's a pretty good way of describing them altogether. In this case, the average is 3, so let's plug this into our system:

$$
2 = 3 + \epsilon_i \\
4 = 3 + \epsilon_i \\
8 = 3 + \beta_{1}*1 + \epsilon_i \\
10 = 3 + \beta_{1}*1 + \epsilon_i \\
$$

Next, we have $\beta_1$. This is the term that indicates what y looks like for the experimental group. Taking a similar approach, we see that, in comparison to the average for the control group (3), the average for the experimental group (9) is 6 units higher. So, we can assign $\beta_1$ a value of 6.

$$
2 = 3 + \epsilon_i \\
4 = 3 + \epsilon_i \\
8 = 3 + 6*1 + \epsilon_i \\
10 = 3 + 6*1 + \epsilon_i \\
$$

Simplifying that, we get:

$$
2 = 3 + \epsilon_i \\
4 = 3 + \epsilon_i \\
8 = 9 + \epsilon_i \\
10 = 9 + \epsilon_i \\
$$

The $\epsilon$ term is called the error, or residuals. It's the variance in y that we haven't yet explained. The form of these residuals is very important for establishing that our quantitative description of the data (i.e. the equations we're using - also known as the model) is a good one. For now, we'll note that they should be normally distributed. In our toy example, they take the values:

```{r}
resids <- c(-1, 1, -1, 1)
cat(resids, sep='\n')
```

Plugging those in, we see that we've solved the equations

$$
2 = 3 - 1 \\
4 = 3 + 1 \\
8 = 9 - 1 \\
10 = 9 + 1 \\
$$

Okay, so why did we go through all this trouble? Well, if we understand the way that these models work, then it makes it easy to generate fake data. Let's do that.

First, let's go back to the fake data we just used. We ended up deciding on a model that looks like this:

$$
y_i = \beta_0 + \beta_{1}X_i + \epsilon_i
$$

Where 

$$
\beta_{0} = 3, \\
\beta_{1} = 6 \\
$$

And we said that our residuals were normally distributed. That implies that those residuals have a mean and some measure of variance. If we get the standard deviation of our residuals from above, we get:

```{r}
sd(resids)
```

which is really far too small. Psychological data are much noisier than that. Let's use a more realistic number for our variability. We'll have our residuals described as a normal distribution with a mean of zero (as it should always have) and a standard deviation of 10.

In other words:

$$
\epsilon = N(0, 10)
$$

That epsilon term is key in letting us generate random data. Without it, we have a completely determined function that describes our dependent variable exactly:

$$
y_i = 3 + 6*X_i
$$

So, it should be clear that we can generate some fake data by taking the following steps:

1. Decide how many individuals are in the control and how many are in the experimental group
2. Generate their perfectly determined data
3. Add some noise as described by the residuals

And here are those steps and a plot of the data:

```{r}
library(ggplot2)
#20 in each cell
X <- rep(c(0, 1), each=20)
#generate Y
Y <- 3 + 6*X
#add noise
Y <- Y + rnorm(40, 0, 10)

df <- data.frame(X=X, Y=Y)

ggplot(df, aes(x=Y)) +
  geom_density(aes(fill=factor(X)), alpha=.3) + 
  scale_fill_manual(values=c('#144256', '#88301B')) + 
  theme_bw()
```

