---
title: "Simulating correlated predictors"
output: html_document
---

The approach I have taken to simulation in this space so far has been pretty simple:

1. Set some values for the intercept and slope of a regression equation.
2. Generate the outcomes as a function of that equation.
3. Add some noise.

And voila - simulation!

However, sometimes we want to simulate something a little bit more complex. For instance, we may want to simulate data from a multivariate model in which some of the variables are correlated with each other in very specific ways. While the above approach will take you a long way, simulating these complex relationships would be very difficult using the simple methods I've been describing. Fortunately, we can use some alternative procedures to simulate data that fits these criteria.

For instance, let's say that we want to generate some data that is a function of three continuous variables. Let's further suppose that these three "predictor" variables are also correlated with each other. How can we do such a thing?

Well, it's helpful in this case to understand the concept of a variance-covariance matrix. Using such a matrix, we can easily and flexibly specify how related we want our variables to be. For instance, if we have three variables, our variance-covariance matrix would look like this:

$$\left[\begin{array}
{rrr}
Var_{1}   & Cov_{1,2} & Cov_{1,3} \\
Cov_{1,2} & Var_{2}   & Cov_{2,3} \\
Cov_{1,3}  & Cov_{2,3}  & Var_{3}
\end{array}\right]
$$

The variance for each variable lies along the diagonal, while the covariance between variables lie on the off-diagonal. We can see also that the matrix is symmetric, with the values below the diagonal mirrored by the values above the diagonal. For our purposes, we can use the scaled version of these measures, which means that the variance-covariance matrix is going to have 1's along the diagonal and correlations on the off-diagonal. 

Regardless, once we've generated our matrix, we can use it to constrain our randomly generated predictor variables to exhibit the properties of correlation that we've decided upon. R has great tools for accomplishing this.

First, let's create our matrix. We're going to have three variables - X, Y, and Z. X and Y will not be correlated, X and Z will be sligthly correlated, and Y and Z will be strongly correlated.

```{r}
library(compiler)
library(corpcor)
library(MASS)

set.seed(42)
n<-100

#correlations between predictors
VCV <- matrix(c(1, 0, .2,
                0, 1, .7,
                .2, .7, 1), nrow=3, ncol=3)
rownames(VCV) <- c('X', 'Y', 'Z')
VCV
```

Great, having done this, we can use the 'mvrnorm' function to turn this into observed variables. This function takes three arguments - the number of observations, the mean of each variable, and the matrix we just created and returns vectors of observations for each variable you're generating.  Here's an example:

```{r}
library(GGally)
library(ggplot2)
dat <- as.data.frame(mvrnorm(n = n, mu = rep(0, 3), Sigma = VCV))
ggpairs(dat, lower=list(params=c(color='#144256')), 
        diag=list(params=c(fill='#144256'))) + theme_bw()
```

We can see that the correlation coefficients (indicated in the upper part of the figure) are in the neighborhood of what we wanted. This is further illustrated by the scatterplots in the lower bit of the figure. 

We'll continue with the series of posts on simulation next time, taking this approach a step or two closer to running a full simulation study.

This post was helped along considerably by a very instructive page on simulating multilevel data on the [UCLA website](http://www.ats.ucla.edu/stat/r/pages/mesimulation.htm).

```{r, eval=FALSE}
library(MASS)

#create predictors
n=100
sigma <- matrix(c(1, .2, .1,
                  .2, 1, .3,
                  .1, .3, 1), ncol=3)
means <- c(0, 0, 0)
dat <- mvrnorm(n, means, sigma)

#investigate data
cor(dat)
colMeans(dat)
pairs(~dat[,1]+dat[,2]+dat[,3])

beta0 <- 0
beta1 <- 1 
beta2 <- 1
beta3 <- 1

Y <- beta0 + beta1*dat[,1] - beta2*dat[,2] + beta3*dat[,3] + rnorm(100, 0, .25)

df <- data.frame(y=Y, x1=dat[,1], x2=dat[,2], x3=dat[,3])

cor(df)
colMeans(df)
pairs(~y+x1+x2+x3, data=df)

summary(lm(y~x1+x2+x3, data=df))

dmat <- cmpfun(function(i) {
    j <- length(i)
    n <- sum(i)
    index <- cbind(start = cumsum(c(1, i[-j])), stop = cumsum(i))
    H <- matrix(0, nrow = n, ncol = j)
    for (i in 1:j) {
        H[index[i, 1]:index[i, 2], i] <- 1L
    }
    return(H)
})


```

```{r, eval=FALSE}
library(compiler)
library(lme4)
library(MASS)
library(corpcor)
dmat <- cmpfun(function(i) {
    j <- length(i)
    n <- sum(i)
    index <- cbind(start = cumsum(c(1, i[-j])), stop = cumsum(i))
    H <- matrix(0, nrow = n, ncol = j)
    for (i in 1:j) {
        H[index[i, 1]:index[i, 2], i] <- 1L
    }
    return(H)
})

r <- cmpfun(function(n, mu, sigma, data) {
    dmat(n) %*% rnorm(length(n), mu, sigma) * data
})

logit <- cmpfun(function(xb) 1/(1 + exp(-xb)))

mycut <- cmpfun(function(x, p) {
    cut(x = x, breaks = quantile(x, probs = p), labels = FALSE, include.lowest = TRUE)
})

hgraph <- cmpfun(function(data) {
    oldpar <- par(no.readonly = TRUE)
    on.exit(par(oldpar))

    n <- ncol(data)
    ncol <- ceiling(sqrt(n))
    nrow <- ceiling(n/ncol)
    par(mfrow = c(nrow, ncol))
    if (!is.null(colnames(data))) {
        i <- colnames(data)
    } else {
        i <- seq(from = 1, to = n)
    }
    out <- lapply(i, function(x) {
        if (is.numeric(data[, x])) {
            hist(data[, x], main = x, xlab = "")
        } else barplot(table(data[, x]), main = x, xlab = "", ylab = "Frequency")
    })
    return(invisible(out))
})


## seed for simulation parameters
set.seed(1)

# total number of hospitals
k <- 35

# number of doctors within each hospital
n <- sample(8:15, size = k, TRUE)
# total number of doctors
j <- sum(n)


# number of patients within each doctor
N <- sample(2:40, size = j, TRUE)
# total number of patients
i <- sum(N)

#establish the means of variables
mu <- list(int = 0, experience = 18, cont = c(Age = 5.1, Married = 0,
    FamilyHx = 0, SmokingHx = 0, Sex = 0, CancerStage = 0, LengthofStay = 6,
    WBC = 6000, RBC = 5), bounded = c(BMI = 5.5, IL6 = 4, CRP = 5))

#correlations between predictors
R <- diag(9)
rownames(R) <- names(mu$cont)
R[1, 2] <- 0.3
R[1, 4] <- 0.3
R[1, 6] <- 0.5
R[1, 7] <- 0.5
R[2, 6] <- -0.2
R[2, 7] <- -0.4
R[2, 8] <- 0.25
R[3, 4] <- -0.5
R[3, 6] <- 0.3
R[3, 7] <- 0.3
R[4, 5] <- 0.3
R[4, 6] <- 0.7
R[4, 7] <- 0.5
R[6, 7] <- 0.5
R[7, 8] <- -0.3
R[8, 9] <- -0.1
R[lower.tri(R)] <- t(R)[lower.tri(t(R))]
(R <- cov2cor(make.positive.definite(R)))

#probabilities
p <- list(school = 0.25, sex = 0.4, married = 0.6, familyhx = 0.2,
    smokehx = c(0.2, 0.2, 0.6), stage = c(0.3, 0.4, 0.2, 0.1))

set.seed(84361)

## hospital variables
b <- cbind(HID = 1:k, Hint = rnorm(k, mean = mu$int, sd = 1), Medicaid = runif(k,
    min = 0.1, max = 0.85))
H <- dmat(N) %*% dmat(n) %*% b #making the matrices the same size

set.seed(50411)
## doctor variables
b <- cbind(DID = 1:j, Dint = rnorm(j, mean = mu$int, sd = 1), Experience = experience <- floor(rnorm(j,
    mean = mu$experience, sd = 4)), School = school <- rbinom(j, 1, prob = p$school),
    Lawsuits = rpois(j, pmax(experience - 8 * school, 0)/8))
D <- dmat(N) %*% b #making the matricies the same size

## continuous variables
Xc <- as.data.frame(cbind(mvrnorm(n = i, mu = rep(0, 9), Sigma = R),
  sapply(mu$bounded, function(k) rchisq(n = i, df = k)))) #this last bit is because the three variables here are chi square distributed

#below gets the variables on the right scale, bounded by the right values
Xc <- within(Xc, {
  Age <- ((Age/1.6) + mu$cont["Age"]) * 10
  Married <- mycut(Married, c(0, 1-p$married, 1)) - 1
  FamilyHx <- mycut(FamilyHx, c(0, 1-p$familyhx, 1)) - 1
  SmokingHx <- factor(mycut(SmokingHx, c(0, cumsum(p$smokehx))))
  Sex <- mycut(Sex, c(0, 1-p$sex, 1)) - 1
  CancerStage <- factor(mycut(CancerStage, c(0, cumsum(p$stage))))
  LengthofStay <- floor(LengthofStay + mu$cont["LengthofStay"])
  WBC <- ((WBC/.001) + mu$cont["WBC"])
  RBC <- ((RBC/3.5) + mu$cont["RBC"])
  BMI <- pmin(BMI * 2 + 18, 58)
})

## create dummies and drop the intercept
Xmdummy <- model.matrix(~ 1 + SmokingHx + CancerStage, data = Xc)[, -1]

#this includes interaction variables.
X <- cbind(Xc[, -c(4,6)], Xmdummy,
  "Sex:Married" = Xc[, "Sex"] * Xc[, "Married"],
  "IL6:CRP" = Xc[, "IL6"] * Xc[, "CRP"],
  "BMI:FamilyHx" = Xc[, "BMI"] * Xc[, "FamilyHx"],
  "SmokingHx2:FamilyHx" = Xmdummy[, "SmokingHx2"] * Xc[, "FamilyHx"],
  "SmokingHx3:FamilyHx" = Xmdummy[, "SmokingHx3"] * Xc[, "FamilyHx"],
  "SmokingHx2:Age" = Xmdummy[, "SmokingHx2"] * Xc[, "Age"],
  "SmokingHx3:Age" = Xmdummy[, "SmokingHx3"] * Xc[, "Age"],
  "Experience:CancerStage2" = D[, "Experience"] * Xmdummy[, "CancerStage2"],
  "Experience:CancerStage3" = D[, "Experience"] * Xmdummy[, "CancerStage3"],
  "Experience:CancerStage4" = D[, "Experience"] * Xmdummy[, "CancerStage4"])

## Final data
mldat <- data.frame(Xc, D, H)
mldat <- mldat[, -which(colnames(mldat) %in% c("Dint", "Hint"))]

#factor data
mldat <- within(mldat, {
  Sex <- factor(Sex, labels = c("female", "male"))
  FamilyHx <- factor(FamilyHx, labels = c("no", "yes"))
  SmokingHx <- factor(SmokingHx, labels = c("current", "former", "never"))
  CancerStage <- factor(CancerStage, labels = c("I", "II", "III", "IV"))
  School <- factor(School, labels = c("average", "top"))
  DID <- factor(DID)
  HID <- factor(HID)
})

#matrix for simulation with interactions
dat <- cbind(X, D, H)
dat <- as.matrix(dat[, -which(colnames(dat) %in% c("DID", "HID"))])

#predictor matrix - how strongly does each row predict each outcome (as indicated by colnames below)
b <- as.data.frame(rbind(
  'Age' = c(1, 1, 0, 0, 1, 8, .8, -1, -1, 0),
  'Married' = c(0, 0, 0, 1, 0, 0, 0, 0, 0, 0),
  'FamilyHx' = c(1, 1, 0, 0, 0, -8, 0, 0, -5, 0),
  'Sex' = c(0, 0, -1, 1, 0, 0, 0, -1, 0, 0),
  'LengthofStay' = c(0, 0, 0, 0, 0, 0, 0, .9, 0, 0),
  'WBC' = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
  'RBC' = c(0, 0, 0, 0, 0, 0, 0, 0, 0, -2),
  'BMI' = c(0, 0, 0, 0, 1, 1, 0, 0, 0, 0),
  'IL6' = c(0, 0, 1, 0, 0, 3.2, 0, 0, -1, 0),
  'CRP' = c(0, 0, 1, 0, 0, 5, 0, 0, -.8, 0),
  'SmokingHx2' = c(-1, -1, 0, 0, 0, -2, -2, 0, 0, 0),
  'SmokingHx3' = c(-2, -2, 0, 0, 0, -10, -4, 0, 0, 0),
  'CancerStage2' = c(1, 1, 0, 0, 0, .5, 2, 1, -1, 0),
  'CancerStage3' = c(1.5, 1.5, 0, 0, 0, 1, 4, 2, -3, 0),
  'CancerStage4' = c(2, 2, 0, 0, 0, 2, 6, 3, -6, 0),
  'Sex:Married' = c(0, 0, 0, 4, 0, 0, 0, 0, 0, 0),
  'IL6:CRP' = c(0, 0, 3, 0, 0, 0, 0, 0, 0, 5),
  'BMI:FamilyHx' = c(0, 0, 0, 0, 0, 60, 0, 0, 0, 0),
  'SmokingHx2:FamilyHx' = c(0, 0, 0, 0, 0, 0, 0, 0, 0, -.5),
  'SmokingHx3:FamilyHx' = c(0, 0, 0, 0, 0, 0, 0, 0, 0, -5),
  'SmokingHx2:Age' = c(-4, -4, 0, 0, 0, 0, 0, 0, 0, 0),
  'SmokingHx3:Age' = c(-8, -8, 0, 0, 0, 0, 0, 0, 0, 0),
  'Experience:CancerStage2' = c(0, 0, 0, 0, 0, 4, 0, 0, 0, 0),
  'Experience:CancerStage3' = c(0, 0, 0, 0, 0, 16, 0, 0, 0, 0),
  'Experience:CancerStage4' = c(0, 0, 0, 0, 0, 40, 0, 0, 0, 0),
  'Dint' = c(3, 3, 3, 3, 3, 20, 8, 10, 5, 10),
  'Experience' = c(0, 0, 0, -5, -3, 0, 0, 0, 3, 2),
  'School' = c(0, 0, 0, 0, 2, 0, 0, 0, 0, 1),
  'Lawsuits' = c(0, 0, 0, 0, -2, 0, 0, 0, 0, 0),
  'Hint' = c(0, 0, 0, 4, 5, 40, 0, 0, 6, 0),
  'Medicaid' = c(0, 0, 0, 0, 3, 0, 0, 0, 0, 0)
))

#predicted units are standardized such that each coefficient represents 1 standard deviation
b <- b / apply(dat, 2, sd)
colnames(b) <- c("tumor", "co2", "pain", "wound", "mobility", "ntumors",
  "zeroinflation", "nmorphine", "remission", "lungcapacity")

set.seed(82231)
#get the expected value of the outcome for tumors by multiplying the data matrix by the coefficients vector
# i.e. rnorm(n=i, mean=dat%*%b$tumor)
# then add the random effects component
# i.e. r(N, .8, .8, dat[, 'LengthofStay'])
#then add 70 to get it centered at the right value
#then give it some noise: sd=8
outcome <- data.frame(tumorsize = rnorm(n = i, mean = (dat %*% b$tumor) +
    r(N, 0.8, 0.8, dat[, "LengthofStay"]) + 70, sd = 8))
hist(outcome$tumorsize, xlab = "", main = "Tumor Size (in mm)")


#simulation code modified from http://www.ats.ucla.edu/stat/r/pages/mesimulation.htm
mycut <- cmpfun(function(x, p) {
    cut(x = x, breaks = quantile(x, probs = p), labels = FALSE, include.lowest = TRUE)
})

set.seed(42)
n<-100

#establish means of predictors
mus <- list(pre=0, cond=0, group=0)
#correlations between predictors
R <- diag(3)
rownames(R) <- names(mus)
R[1,2] <- 0
R[1,3] <- 0
R[2,3] <- 0
R[lower.tri(R)] <- t(R)[lower.tri(t(R))]
(R <- cov2cor(make.positive.definite(R)))

#probabilities per grouping variable:
p <- list(cond=.5, group=.5)

X <- as.data.frame(mvrnorm(n=n, mu=rep(0,3), Sigma=R))

#convert grouping variables
X <- within(X, {
  cond <- mycut(cond, c(0, 1-p$cond, 1)) - 1
  group <- mycut(group, c(0, 1-p$cond, 1)) - 1
})

#Create interaction variables
Xints <- cbind(X,
  "pre:cond" = X[, "pre"] * X[, "cond"],
  "pre:group" = X[, "pre"] * X[, "group"],
  "cond:group" = X[, "cond"] * X[, "group"],
  "pre:cond:group" = X[, "pre"] * X[, "cond"] * X[, "group"])

#factors
df <- within(Xints, {
  cond <- factor(cond, labels=c('control', 'intervention'))
  group <- factor(group, labels=c('nonstereotyped', 'stereotyped'))
})

dfmat <- as.matrix(Xints)

#predictor vector
b <- as.data.frame(rbind(
  'pre' = 1,
  'cond' = .05,
  'group' = -.5,
  'pre:cond' = 0,
  'pre:group' = 0,
  'cond:group' = .5,
  'pre:cond:group' = 0
))

#standardize
b <- b/apply(dfmat, 2, sd)
colnames(b) <- 'post'

outcome <- data.frame(post=rnorm(n=n, mean=(dfmat %*% b$post), sd=1))
```



ourData <- data.frame(myData, yourData, hisData, herData)

crv<-exp(mean(log(Cov))+sd(log(Cov))*qnorm(.5)) #see Tannenbaum, Holford, Lee, Peck & Mould (2006)
cat.cov <- ifelse(Cov > crv, 1, 0)

# interactions! http://www.ats.ucla.edu/stat/r/pages/mesimulation.htm



cov(ourData)
```


