{
 "metadata": {
  "name": "",
  "signature": "sha256:75ae9c7bb660abd6775e30617ec798f45eded776e5307f23f4a034116ba81f34"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In some previous posts, I used some data scraped from the New York Posts' astrological horoscopes to demonstrate that the text of a horoscope is a better indicator of the month in which it was written than the astrological sign it was written for. Here are the results from attempting to classify the horoscope texts into their astrological signs:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import svm\n",
      "from sklearn import metrics\n",
      "\n",
      "df = pd.read_csv('../../../data/astrosign.csv', sep='|')\n",
      "df = df.drop('Unnamed: 0', 1)\n",
      "df=df.dropna()\n",
      "df['month'] = df['pub_date'].map(lambda x: str(x)[0:2])\n",
      "\n",
      "cv = CountVectorizer()\n",
      "wordcounts = cv.fit_transform(df['horoscope'])\n",
      "scope_train, scope_test, sign_train, sign_true = \\\n",
      "    train_test_split(wordcounts, \n",
      "                     df['zodiac'], \n",
      "                     test_size=.3, \n",
      "                     random_state=42)\n",
      "\n",
      "clf = svm.LinearSVC()\n",
      "clf.fit(scope_train, sign_train)\n",
      "\n",
      "predicted = clf.predict(scope_test)\n",
      "scores = metrics.classification_report(sign_true, predicted)\n",
      "print scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "   aquarius       0.09      0.07      0.08       176\n",
        "      aries       0.12      0.14      0.13       138\n",
        "     cancer       0.00      0.00      0.00       173\n",
        "  capricorn       0.07      0.08      0.08       148\n",
        "     gemini       0.00      0.00      0.00       155\n",
        "        leo       0.13      0.10      0.12       163\n",
        "      libra       0.17      0.15      0.16       155\n",
        "     pisces       0.13      0.13      0.13       147\n",
        "sagittarius       0.13      0.13      0.13       145\n",
        "    scorpio       0.18      0.17      0.17       168\n",
        "     taurus       0.00      0.00      0.00       145\n",
        "      virgo       0.10      0.08      0.09       169\n",
        "\n",
        "avg / total       0.09      0.09      0.09      1882\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see, the performance here is not so good. We're essentially performing at chance when examining the performance as a whole.\n",
      "\n",
      "However, when we look at classifying on month, performance is much better:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = CountVectorizer()\n",
      "wordcounts = cv.fit_transform(df['horoscope'])\n",
      "scope_train, scope_test, month_train, month_true = \\\n",
      "    train_test_split(wordcounts, \n",
      "                     df.month, \n",
      "                     test_size=.3, \n",
      "                     random_state=42)\n",
      "    \n",
      "clf = svm.LinearSVC()\n",
      "clf.fit(scope_train, month_train)\n",
      "\n",
      "predicted = clf.predict(scope_test)\n",
      "scores = metrics.classification_report(month_true, predicted)\n",
      "print scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "         01       0.32      0.33      0.33       203\n",
        "         02       0.35      0.33      0.34       210\n",
        "         03       0.31      0.37      0.34       207\n",
        "         04       0.27      0.33      0.30       190\n",
        "         05       0.28      0.26      0.27       186\n",
        "         06       0.32      0.32      0.32       117\n",
        "         07       0.31      0.21      0.25       126\n",
        "         08       0.24      0.26      0.25       108\n",
        "         09       0.37      0.28      0.32       113\n",
        "         10       0.36      0.31      0.34       102\n",
        "         11       0.32      0.29      0.31       102\n",
        "         12       0.31      0.31      0.31       218\n",
        "\n",
        "avg / total       0.31      0.31      0.31      1882\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can probably do a bit better here, because we know that there are a greater number of horoscopes from some months than from others (because I've scraped a year and a half's worth of data)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scope_train, scope_test, month_train, month_true = \\\n",
      "    train_test_split(wordcounts, \n",
      "                     df.month, \n",
      "                     test_size=.3, \n",
      "                     random_state=42)\n",
      "    \n",
      "clf = svm.LinearSVC(class_weight='auto')\n",
      "clf.fit(scope_train, month_train)\n",
      "\n",
      "predicted = clf.predict(scope_test)\n",
      "scores = metrics.classification_report(month_true, predicted)\n",
      "print scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "         01       0.33      0.33      0.33       203\n",
        "         02       0.36      0.34      0.35       210\n",
        "         03       0.31      0.36      0.33       207\n",
        "         04       0.27      0.32      0.29       190\n",
        "         05       0.29      0.27      0.28       186\n",
        "         06       0.31      0.31      0.31       117\n",
        "         07       0.30      0.22      0.26       126\n",
        "         08       0.24      0.28      0.26       108\n",
        "         09       0.33      0.28      0.31       113\n",
        "         10       0.34      0.32      0.33       102\n",
        "         11       0.30      0.30      0.30       102\n",
        "         12       0.32      0.31      0.32       218\n",
        "\n",
        "avg / total       0.31      0.31      0.31      1882\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "#the RF classifier doesn't take the sparse numpy array we used before, \n",
      "#so we just have to turn it into a regular array. This doesn't change \n",
      "#the values at all, it just changes the internal representation.\n",
      "wcarray = wordcounts.toarray()\n",
      "\n",
      "clf = RandomForestClassifier(class_weight='auto')\n",
      "clf.fit(scope_train, month_train)\n",
      "predicted = clf.predict(scope_test)\n",
      "scores = metrics.classification_report(month_true, predicted)\n",
      "print scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "__init__() got an unexpected keyword argument 'class_weight'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-15-9226c2a12686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                      random_state=42)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'class_weight'"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I would say that this pretty convincingly shows that there's more information in the horoscopes that pertains to the month of the year in which it was published than the astrological sign.\n",
      "\n",
      "Just to be complete, let's use a random forest as well, just like we tried in the last post."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.preprocessing import balance_weights\n",
      "\n",
      "#the RF classifier doesn't take the sparse numpy array we used before, \n",
      "#so we just have to turn it into a regular array. This doesn't change \n",
      "#the values at all, it just changes the internal representation.\n",
      "wcarray = wordcounts.toarray()\n",
      "\n",
      "scope_train, scope_test, month_train, month_true = \\\n",
      "    train_test_split(wcarray, \n",
      "                     df.month, \n",
      "                     test_size=.3, \n",
      "                     random_state=42)\n",
      "    \n",
      "clf = RandomForestClassifier()\n",
      "clf.fit(scope_train, month_train, sample_weight=balance_weights(month_train))\n",
      "predicted = clf.predict(scope_test)\n",
      "scores = metrics.classification_report(month_true, predicted)\n",
      "print scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "         01       0.22      0.46      0.30       203\n",
        "         02       0.32      0.37      0.34       210\n",
        "         03       0.22      0.35      0.27       207\n",
        "         04       0.25      0.35      0.29       190\n",
        "         05       0.29      0.25      0.27       186\n",
        "         06       0.67      0.28      0.40       117\n",
        "         07       0.57      0.21      0.31       126\n",
        "         08       0.49      0.19      0.28       108\n",
        "         09       0.57      0.24      0.34       113\n",
        "         10       0.60      0.28      0.39       102\n",
        "         11       0.51      0.25      0.33       102\n",
        "         12       0.36      0.27      0.31       218\n",
        "\n",
        "avg / total       0.38      0.31      0.31      1882\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/travis/anaconda/lib/python2.7/site-packages/sklearn/utils/__init__.py:93: DeprecationWarning: Function balance_weights is deprecated; balance_weights is an internal function and will be removed in 0.16\n",
        "  warnings.warn(msg, category=DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A random forest seems to give us a bit better precision in this case, but the f1 score is the same. There's a problem here, however. Unlike when we were using horoscopes, our classes are not roughly equivalent in terms of the number of instances. Specifically, there are fewer cases for the months of June through November. This could be (and almost certainly is) biasing our learner and is an important factor to consider when fitting these kinds of models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}